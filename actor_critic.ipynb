{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhuqIQECd+9WQUDccZfcOB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Calcifer777/learn-rl/blob/main/actor_critic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "apt install -q xvfb python-opengl ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U08uRz2Eu_t9",
        "outputId": "4fa00a03-1064-403a-fd88-5ac3e5882934"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "python-opengl is already the newest version (3.1.0+dfsg-2build1).\n",
            "ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n",
            "xvfb is already the newest version (2:1.20.13-1ubuntu1~20.04.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 24 not upgraded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip -q install gymnasium gym-notebook-wrapper renderlab"
      ],
      "metadata": {
        "id": "FjOCLcV1DnZU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "237Q04Y1DSSi"
      },
      "outputs": [],
      "source": [
        "from collections import namedtuple\n",
        "from datetime import datetime\n",
        "import logging\n",
        "from typing import List\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "from matplotlib import pyplot as plt\n",
        "import renderlab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 42\n",
        "DISCOUNT = 0.99\n",
        "NUM_EPISODES = 1000\n",
        "NUM_LOGS = 20\n",
        "LOG_INTERVAL = NUM_EPISODES // NUM_LOGS\n",
        "T = 500\n",
        "RUNNING_REWARD_COEF = 0.05\n",
        "EPS = np.finfo(np.float32).eps.item()\n",
        "LR = 2e-3\n",
        "\n",
        "LOG_TEMPLATE = (\n",
        "    \"Episode {ep_idx:03}\\t\"\n",
        "    \"Last reward: {ep_reward:.2f}\\t\"\n",
        "    \"Average reward: {running_reward:.2f}\"\n",
        ")"
      ],
      "metadata": {
        "id": "imp5OdfUH-Rs"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SavedAction = namedtuple(\"SavedAction\", [\"log_prob\", \"value\"])"
      ],
      "metadata": {
        "id": "Xjh55P0_Gdzy"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Policy(nn.Module):\n",
        "  def __init__(self, inputs_dim, hidden_dim, outputs_dim):\n",
        "    super(Policy, self).__init__()\n",
        "    self.l1 = nn.Linear(inputs_dim, out_features=hidden_dim)\n",
        "    self.l2 = nn.Linear(hidden_dim, out_features=hidden_dim)\n",
        "    \n",
        "    self.action_head = nn.Linear(hidden_dim, outputs_dim)\n",
        "    self.value_head = nn.Linear(hidden_dim, 1)\n",
        "    \n",
        "    self.saved_actions: List[SavedAction] = []\n",
        "    self.rewards = []\n",
        "\n",
        "  def forward(self, x):\n",
        "    h = self.l1(x)\n",
        "    h = F.relu(h)\n",
        "    h = self.l2(h)\n",
        "    action_probs = F.softmax(self.action_head(h), dim=-1)\n",
        "    values = self.value_head(h)\n",
        "    return action_probs, values"
      ],
      "metadata": {
        "id": "ByGpDJzlDkko"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test model\n",
        "policy = Policy(inputs_dim=4, hidden_dim=64, outputs_dim=2)\n",
        "sample = torch.rand((1, 4))\n",
        "policy(sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DancXfkEFUeM",
        "outputId": "69fb480e-3a21-40c0-bbba-e86e029cc2fe"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.4331, 0.5669]], grad_fn=<SoftmaxBackward0>),\n",
              " tensor([[0.0114]], grad_fn=<AddmmBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def select_action(policy, state):\n",
        "  state = torch.from_numpy(state).float()\n",
        "  probs, value = policy(state)\n",
        "\n",
        "  action_distr = Categorical(probs)\n",
        "  action = action_distr.sample()\n",
        "\n",
        "  policy.saved_actions.append(\n",
        "      SavedAction(action_distr.log_prob(action), value)\n",
        "  )\n",
        "\n",
        "  return action.item()"
      ],
      "metadata": {
        "id": "aaojeS0rFqJ1"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "env.reset(seed=RANDOM_SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cE2yn-IHWxP",
        "outputId": "6c3447ee-37f1-4723-c22a-3023f443b0b8"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0.0273956 , -0.00611216,  0.03585979,  0.0197368 ], dtype=float32),\n",
              " {})"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test action selection\n",
        "state, _ = env.reset()\n",
        "action = select_action(policy, state)\n",
        "env.step(action)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTBZjufNHw2b",
        "outputId": "9a28ee6f-74cb-46b3-fa0b-7aaa73b5cde9"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([-0.03963102, -0.14792429,  0.0266861 ,  0.32941288], dtype=float32),\n",
              " 1.0,\n",
              " False,\n",
              " False,\n",
              " {})"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_episode(env, policy: Policy, max_time_steps: int=T):\n",
        "\n",
        "  state, _ = env.reset()\n",
        "  ep_reward = 0\n",
        "\n",
        "  for t in range(max_time_steps):\n",
        "    action = select_action(policy, state)\n",
        "    state, reward, done, _, _ = env.step(action)\n",
        "    policy.rewards.append(reward)\n",
        "\n",
        "    ep_reward += reward\n",
        "\n",
        "    if done:\n",
        "      break\n",
        "  \n",
        "  return ep_reward"
      ],
      "metadata": {
        "id": "EcrieoM1JpCj"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test run episode\n",
        "policy = Policy(inputs_dim=4, hidden_dim=64, outputs_dim=2)\n",
        "run_episode(env, policy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viou0IUXpb6r",
        "outputId": "e1083f51-2d38-4bc4-a592-1b903dc6e2af"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16.0"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def backprop(policy: Policy, optimizer):\n",
        "  actions = policy.saved_actions\n",
        "  rewards = policy.rewards\n",
        "\n",
        "  losses_policy, losses_value = [], []\n",
        "\n",
        "  # For each step, compute its value\n",
        "  values = []\n",
        "  accumulated_return = 0\n",
        "  for reward in reversed(policy.rewards):\n",
        "    accumulated_return = reward + DISCOUNT * accumulated_return\n",
        "    values.append(accumulated_return)\n",
        "  values = list(reversed(values))\n",
        "  values = torch.tensor(values)\n",
        "  values = (values - values.mean()) / (values.std() + EPS)\n",
        "\n",
        "  # For each step, compute the loss\n",
        "  for (logit, value_hat), value in zip(actions, values):\n",
        "    advantage = value - value_hat.item()\n",
        "    losses_policy.append(-logit * advantage)\n",
        "    losses_value.append(F.smooth_l1_loss(value_hat, torch.tensor([value])))\n",
        "\n",
        "  loss = (\n",
        "      torch.stack(losses_policy).sum() +\n",
        "      torch.stack(losses_value).sum()\n",
        "  )\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  del policy.saved_actions[:]\n",
        "  del policy.rewards[:]\n",
        "\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "BPkDf59yKBKd"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test backprop\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "env.reset(seed=RANDOM_SEED)\n",
        "\n",
        "policy = Policy(inputs_dim=4, hidden_dim=64, outputs_dim=2)\n",
        "optimizer = optim.Adam(policy.parameters(), 1e-2)\n",
        "run_episode(env, policy)\n",
        "backprop(policy, optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO-5YNg2OS-Z",
        "outputId": "691aefba-287c-4c84-8409-ef615885c49c"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(11.9012, grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(RANDOM_SEED)\n",
        "\n",
        "# Test backprop\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "env.reset(seed=RANDOM_SEED)\n",
        "\n",
        "policy = Policy(inputs_dim=4, hidden_dim=128, outputs_dim=2)\n",
        "optimizer = optim.Adam(policy.parameters(), LR)"
      ],
      "metadata": {
        "id": "1o0Y2GChHcZt"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "rewards = []\n",
        "\n",
        "running_reward = 0\n",
        "\n",
        "for ep_idx in range(NUM_EPISODES):\n",
        "\n",
        "  ep_reward = run_episode(env, policy)\n",
        "  rewards.append(ep_reward)\n",
        "  \n",
        "  loss = backprop(policy, optimizer)\n",
        "  losses.append(loss.item())\n",
        "\n",
        "  running_reward = (\n",
        "      RUNNING_REWARD_COEF * ep_reward + \n",
        "      (1-RUNNING_REWARD_COEF) * running_reward\n",
        "  )\n",
        "  if ep_idx % LOG_INTERVAL == 0:\n",
        "    log_txt = LOG_TEMPLATE.format(\n",
        "      ep_idx=ep_idx, \n",
        "      ep_reward=ep_reward, \n",
        "      running_reward=running_reward,\n",
        "    )\n",
        "    logging.warning(log_txt)\n",
        "\n",
        "  if running_reward > env.spec.reward_threshold:\n",
        "    logging.warning(\n",
        "      f\"Solved! Running reward is now {running_reward} and \"\n",
        "      f\"the last episode runs to {ep_idx} time steps!\"\n",
        "    )\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "vn53JxeEH813",
        "outputId": "2e5eaeb9-9af5-460a-bdcb-6ebb7dd72f83"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Episode 000\tLast reward: 29.00\tAverage reward: 1.45\n",
            "WARNING:root:Episode 050\tLast reward: 32.00\tAverage reward: 27.33\n",
            "WARNING:root:Episode 100\tLast reward: 139.00\tAverage reward: 86.42\n",
            "WARNING:root:Episode 150\tLast reward: 141.00\tAverage reward: 164.33\n",
            "WARNING:root:Episode 200\tLast reward: 205.00\tAverage reward: 213.81\n",
            "WARNING:root:Episode 250\tLast reward: 484.00\tAverage reward: 377.03\n",
            "WARNING:root:Episode 300\tLast reward: 500.00\tAverage reward: 407.82\n",
            "WARNING:root:Episode 350\tLast reward: 500.00\tAverage reward: 359.10\n",
            "WARNING:root:Episode 400\tLast reward: 241.00\tAverage reward: 251.19\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-0404e22604b3>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mep_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-dbf1e4846a50>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(env, policy, max_time_steps)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_time_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-75-0fa8b692c92c>\u001b[0m in \u001b[0;36mselect_action\u001b[0;34m(policy, state)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_distr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   policy.saved_actions.append(\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0mSavedAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_distr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def running_mean(x, N = 50):\n",
        "  kernel = np.ones(N)\n",
        "  conv_len = x.shape[0]-N\n",
        "  y = np.zeros(conv_len)\n",
        "  for i in range(conv_len):\n",
        "    y[i] = kernel @ x[i:i+N]\n",
        "    y[i] /= N\n",
        "  return y"
      ],
      "metadata": {
        "id": "lTnH6cU8uf8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = np.array(rewards)\n",
        "plt.figure(figsize=(15,7))\n",
        "plt.ylabel(\"Trajectory Duration\",fontsize=12)\n",
        "plt.xlabel(\"Training Epochs\",fontsize=12)\n",
        "plt.plot(score, color='gray' , linewidth=1)\n",
        "plt.scatter(\n",
        "  np.arange(score.shape[0]),\n",
        "  score, \n",
        "  color='green', \n",
        "  linewidth=0.3\n",
        ")\n",
        "\n",
        "window = 50\n",
        "avg_score = running_mean(score, window)\n",
        "plt.plot(np.arange(len(avg_score))+window, avg_score, color='blue', linewidth=3)"
      ],
      "metadata": {
        "id": "Mirdoc25sr10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts = datetime.utcnow().isoformat()\n",
        "env_rendered = renderlab.RenderFrame(env, directory=f\"./actor-critic-{ts}\")\n",
        "run_episode(env_rendered, policy)\n",
        "env_rendered.play()"
      ],
      "metadata": {
        "id": "hAze9ralu7XO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4v9wFaTDvxWL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}